{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDb_web_scrape.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTxbVTjONahw2Z/eQ081Mg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahul30032/IMDb_web_scraper/blob/main/IMDb_web_scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsyF8ozIKe-f"
      },
      "source": [
        "## Internship Selection Task - MedBikri - web scraping \n",
        "\n",
        "* Task Description - Scraping Multiplle Pages of of websites to obtain formatted data into a csv.\n",
        "* website URL - [IMDb \"Top 1000\" (Sorted by Popularity Ascending)](https://www.imdb.com/search/title/?groups=top_1000&ref_=adv_prv) \n",
        "* Tech used - Python with bs4(BeautifulSoup) for scraping \n",
        "* Note : Following code was written on google colab and with appropriate changes can be run on local system as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQAhwx_-lzK8"
      },
      "source": [
        "from google.colab import drive   # for colab-drive integratiion "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q0tdrAHl1Qe",
        "outputId": "c552e53f-11a2-405e-e3af-9c73cc42a771"
      },
      "source": [
        "drive.mount('/content/gdrive')  # mounting the gdrive "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm39i0O8L6kv"
      },
      "source": [
        "# importing all necessary tools \n",
        "\n",
        "import requests\n",
        "from requests import get\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from time import sleep\n",
        "from random import randint"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYyEbb5kMF3w"
      },
      "source": [
        "# intialising all the lists we gonna store our scraped data to \n",
        "titles = []                       # movie titles \n",
        "years = []                        # year of release \n",
        "time = []                         # total run-time \n",
        "imdb_ratings = []                 # IMDb ratings \n",
        "metascores = []                   #Metascore \n",
        "votes = []                        # Votes \n",
        "us_gross = []                     # Total gross in USD "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iyin742iMPJA"
      },
      "source": [
        "# ensuring we obtain english translated movie titles only.\n",
        "headers = {\"Accept-Language\": \"en-US,en;q=0.5\"}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df92ALuol_gT"
      },
      "source": [
        "# pages is a URL parameter which helps us move to the next page after scraping through the current page\n",
        "pages = np.arange(1, 1001, 50) # each page shows 50 movie titles and description. and that is reflected in the URL \n",
        "\n",
        "for page in pages: \n",
        "\n",
        "  page = requests.get(\"https://www.imdb.com/search/title/?groups=top_1000&start=\" + str(page) + \"&ref_=adv_nxt\", headers=headers)   \n",
        "\n",
        "  soup = BeautifulSoup(page.text, 'html.parser')\n",
        "  movie_div = soup.find_all('div', class_='lister-item mode-advanced')\n",
        "  \n",
        "  sleep(randint(2,10))    # controlling crawling rate as otherwise speedy \"get\" requests puts presssure on website server \n",
        "\n",
        "  for container in movie_div:\n",
        "\n",
        "        name = container.h3.a.text\n",
        "        titles.append(name)\n",
        "        \n",
        "        year = container.h3.find('span', class_='lister-item-year').text\n",
        "        years.append(year)\n",
        "\n",
        "        runtime = container.p.find('span', class_='runtime') if container.p.find('span', class_='runtime') else ''\n",
        "        time.append(runtime)\n",
        "\n",
        "        imdb = float(container.strong.text)\n",
        "        imdb_ratings.append(imdb)\n",
        "\n",
        "        m_score = container.find('span', class_='metascore').text if container.find('span', class_='metascore') else ''\n",
        "        metascores.append(m_score)\n",
        "\n",
        "        nv = container.find_all('span', attrs={'name': 'nv'})\n",
        "        \n",
        "        vote = nv[0].text\n",
        "        votes.append(vote)\n",
        "        \n",
        "        grosses = nv[1].text if len(nv) > 1 else ''\n",
        "        us_gross.append(grosses)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgHJ2ocMMfKQ"
      },
      "source": [
        "# storing all lists into a dataframe \n",
        "movies = pd.DataFrame({\n",
        "'movie': titles,\n",
        "'year': years,\n",
        "'imdb': imdb_ratings,\n",
        "'metascore': metascores,\n",
        "'votes': votes,\n",
        "'us_grossMillions': us_gross,\n",
        "'timeMin': time\n",
        "})\n",
        "\n",
        "# data cleaning \n",
        "\n",
        "movies['votes'] = movies['votes'].str.replace(',', '').astype(int)  # extracting all digits in the string and converting it to integers \n",
        "\n",
        "movies.loc[:, 'year'] = movies['year'].str[-5:-1].astype(int)\n",
        "\n",
        "movies['timeMin'] = movies['timeMin'].astype(str)\n",
        "movies['timeMin'] = movies['timeMin'].str.extract('(\\d+)').astype(int)\n",
        "\n",
        "movies['metascore'] = movies['metascore'].str.extract('(\\d+)')\n",
        "movies['metascore'] = pd.to_numeric(movies['metascore'], errors='coerce')\n",
        "\n",
        "movies['us_grossMillions'] = movies['us_grossMillions'].map(lambda x: x.lstrip('$').rstrip('M'))  # strips $ from start and M from the end of gross if provided\n",
        "movies['us_grossMillions'] = pd.to_numeric(movies['us_grossMillions'], errors='coerce')\n",
        "\n",
        "\n",
        "# # to see your dataframe\n",
        "# print(movies)\n",
        "\n",
        "# # to see the datatypes of your columns\n",
        "# print(movies.dtypes)\n",
        "\n",
        "# # to see where you're missing data and how much data is missing \n",
        "# print(movies.isnull().sum())\n",
        "\n",
        "# to move all your scraped data to a CSV file\n",
        "movies.to_csv('movies.csv')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtUthhntm5OK"
      },
      "source": [
        "# code to be run specifically on colab, need not run it on local environments like Jupyter Notebook\n",
        "!cp movies.csv \"/content/gdrive/My Drive/imdb_scraping\"   # saves CSV to specified drive location "
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}